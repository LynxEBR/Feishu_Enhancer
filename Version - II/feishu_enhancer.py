"""
é£ä¹¦ä»»åŠ¡å¢å¼ºå™¨ - è”ç½‘æœç´¢ + PostgreSQL å‘é‡ç¼“å­˜
åŸºäº LangChain Agent çš„ ReAct æ¨¡å¼å®ç°
"""
import os
import re
import asyncio
import httpx
from typing import Optional, Dict, Any, List, Tuple
from urllib.parse import urlparse

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain.agents import initialize_agent, Tool, AgentType

# å¯¼å…¥ä¸šåŠ¡çŸ¥è¯†åº“ç›¸å…³æ¨¡å—
from business_knowledge.database import get_db
from business_knowledge.crud import BusinessKnowledgeCRUD


class FeishuTaskEnhancer:
    """
    é£ä¹¦ä»»åŠ¡å¢å¼ºå™¨ï¼ˆåŸºäº LangChain Agentï¼‰
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. è¯­ä¹‰ç¼“å­˜ï¼ˆPostgreSQL å‘é‡æ•°æ®åº“ï¼‰
    2. æ™ºèƒ½æ£€ç´¢ï¼ˆç›¸ä¼¼åº¦ >= é˜ˆå€¼ç›´æ¥è¿”å›ï¼‰
    3. Agent æœç´¢ï¼ˆLangChain ReAct + Bocha APIï¼Œé™å®šé£ä¹¦å®˜ç½‘ï¼‰
    4. è‡ªåŠ¨æ²‰æ·€ï¼ˆæœç´¢ç»“æœè‡ªåŠ¨å…¥åº“ï¼‰
    """
    
    # é£ä¹¦å®˜æ–¹æ–‡æ¡£ URL æ¨¡å¼ï¼ˆç™½åå•ï¼‰
    FEISHU_OFFICIAL_URL_PATTERNS = [
        'https://www.feishu.cn/hc/zh-CN/articles/',
        'https://www.feishu.cn/hc/en-US/articles/',
        'https://www.feishu.cn/hc/zh-CN/',
        'https://www.feishu.cn/hc/en-US/',
        'https://www.larksuite.com/hc/',
        'https://feishu.cn/hc/',
    ]
    
    def __init__(
        self,
        llm: ChatOpenAI,
        bocha_api_key: Optional[str] = None,
        bocha_base_url: str = "https://api.bochaai.com/v1",
        similarity_threshold: float = 0.85,
        enable_cache: bool = True,
        enable_web_search: bool = True,
        verbose: bool = True,
        max_agent_iterations: int = 5
    ):
        """
        åˆå§‹åŒ–å¢å¼ºå™¨
        
        Args:
            llm: LangChain ChatOpenAI å®ä¾‹
            bocha_api_key: Bocha API å¯†é’¥ï¼ˆç”¨äºè”ç½‘æœç´¢ï¼‰
            bocha_base_url: Bocha API åŸºç¡€ URL
            similarity_threshold: ç¼“å­˜å‘½ä¸­çš„ç›¸ä¼¼åº¦é˜ˆå€¼
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            enable_web_search: æ˜¯å¦å¯ç”¨è”ç½‘æœç´¢
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
            max_agent_iterations: Agent æœ€å¤§è¿­ä»£æ¬¡æ•°
        """
        self.llm = llm
        self.bocha_api_key = bocha_api_key or os.getenv("BOCHA_API_KEY", "")
        self.bocha_base_url = bocha_base_url
        self.similarity_threshold = similarity_threshold
        self.enable_cache = enable_cache
        self.enable_web_search = enable_web_search
        self.verbose = verbose
        self.max_agent_iterations = max_agent_iterations
        
        # åˆå§‹åŒ–é£ä¹¦æ–‡æ¡£æœç´¢å·¥å…·
        self._init_search_tool()
        
        if self.verbose:
            print(f"âœ“ FeishuTaskEnhancer åˆå§‹åŒ–å®Œæˆ")
            print(f"  - ç›¸ä¼¼åº¦é˜ˆå€¼: {self.similarity_threshold}")
            print(f"  - ç¼“å­˜å¯ç”¨: {self.enable_cache}")
            print(f"  - è”ç½‘æœç´¢å¯ç”¨: {self.enable_web_search}")
        
    def _get_crud(self) -> BusinessKnowledgeCRUD:
        """è·å–æ•°æ®åº“ CRUD å®ä¾‹"""
        db = next(get_db())
        return BusinessKnowledgeCRUD(db)
    
    def _init_search_tool(self):
        """åˆå§‹åŒ–é£ä¹¦æ–‡æ¡£æœç´¢å·¥å…·ï¼ˆä¾› Agent ä½¿ç”¨ï¼‰"""
        self.feishu_search_tool = Tool(
            name="FeishuDocSearch",
            func=lambda query: self._bocha_feishu_search_sync(query, count=5),
            description="ä¸“é—¨æœç´¢é£ä¹¦å®˜æ–¹å¸®åŠ©æ–‡æ¡£(feishu.cn/hc)ã€‚è¾“å…¥åº”ä¸ºæœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œè¾“å‡ºå°†ä¼˜å…ˆè¿”å›é£ä¹¦å®˜æ–¹æ–‡æ¡£çš„æœç´¢ç»“æœã€‚"
        )
    
    def _is_feishu_official_doc(self, url: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦æ˜¯é£ä¹¦å®˜æ–¹æ–‡æ¡£
        
        Args:
            url: ç½‘é¡µ URL
            
        Returns:
            æ˜¯å¦ä¸ºå®˜æ–¹æ–‡æ¡£
        """
        return any(url.startswith(pattern) for pattern in self.FEISHU_OFFICIAL_URL_PATTERNS)
    
    def _score_feishu_result(self, page: Dict, query: str) -> Tuple[bool, int]:
        """
        å¯¹æœç´¢ç»“æœè¿›è¡Œè¯„åˆ†
        
        è¯„åˆ†è§„åˆ™ï¼š
        - å®˜æ–¹æ–‡æ¡£ (feishu.cn/hc/): +100 åˆ†
        - æ ‡é¢˜åŒ…å«æŸ¥è¯¢è¯: +10 åˆ†
        - æ‘˜è¦åŒ…å«æŸ¥è¯¢è¯: +5 åˆ†
        - URL åŒ…å« /hc/: +20 åˆ†
        - æ‘˜è¦é•¿åº¦é€‚ä¸­ (50-500): +3 åˆ†
        
        Args:
            page: æœç´¢ç»“æœé¡µé¢ä¿¡æ¯
            query: åŸå§‹æŸ¥è¯¢
            
        Returns:
            (æ˜¯å¦å®˜æ–¹æ–‡æ¡£, è¯„åˆ†)
        """
        url = page.get('url', '')
        name = page.get('name', '').lower()
        summary = page.get('summary', page.get('snippet', '')).lower()
        query_lower = query.lower()
        
        is_official = self._is_feishu_official_doc(url)
        score = 0
        
        # å®˜æ–¹æ–‡æ¡£åŠ åˆ†
        if is_official:
            score += 100
        
        # æ ‡é¢˜åŒ¹é…
        if query_lower in name:
            score += 10
        
        # æ‘˜è¦åŒ¹é…
        if query_lower in summary:
            score += 5
        
        # å¸®åŠ©ä¸­å¿ƒè·¯å¾„åŠ åˆ†
        if 'feishu.cn/hc' in url or '/hc/' in url:
            score += 20
        
        # æ‘˜è¦é•¿åº¦é€‚ä¸­åŠ åˆ†
        summary_len = len(summary)
        if 50 < summary_len < 500:
            score += 3
        
        return (is_official, score)
    
    def _bocha_feishu_search_sync(self, query: str, count: int = 5) -> str:
        """
        ä½¿ç”¨ Bocha API æœç´¢é£ä¹¦å®˜æ–¹æ–‡æ¡£ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œä¾› Agent ä½¿ç”¨ï¼‰
        
        å…³é”®æ”¹è¿›ï¼š
        1. ç¡¬ç¼–ç  site:feishu.cn + å¸®åŠ©ä¸­å¿ƒ çº¦æŸ
        2. ç»“æœäºŒæ¬¡æ¸…æ´—ä¸è¯„åˆ†
        3. ä¼˜å…ˆè¿”å›å®˜æ–¹æ–‡æ¡£
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            count: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æ ¼å¼åŒ–çš„æœç´¢ç»“æœå­—ç¬¦ä¸²
        """
        # ã€å…³é”®ã€‘å¼ºåˆ¶é™å®šé£ä¹¦å®˜æ–¹åŸŸå + å¸®åŠ©ä¸­å¿ƒ
        search_query = f"site:feishu.cn å¸®åŠ©ä¸­å¿ƒ {query}"
        
        if self.verbose:
            print(f"[FeishuDocSearch] æœç´¢å…³é”®è¯: {search_query}")
        
        if not self.bocha_api_key:
            return "é”™è¯¯ï¼šæœªé…ç½® Bocha API Key"
        
        import requests
        
        url = f"{self.bocha_base_url}/web-search"
        headers = {
            'Authorization': f'Bearer {self.bocha_api_key}',
            'Content-Type': 'application/json'
        }
        data = {
            "query": search_query,
            "freshness": "noLimit",
            "summary": True,
            "count": 30  # å¤šå–ä¸€äº›ï¼Œç”¨äºç­›é€‰
        }
        
        try:
            response = requests.post(url, headers=headers, json=data, timeout=30)
            
            if response.status_code != 200:
                return f"æœç´¢ API è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}"
            
            json_response = response.json()
            if json_response.get("code") != 200 or not json_response.get("data"):
                return f"æœç´¢ API è¿”å›é”™è¯¯: {json_response.get('msg', 'æœªçŸ¥é”™è¯¯')}"
            
            webpages = json_response["data"].get("webPages", {}).get("value", [])
            
            if not webpages:
                return "æœªæ‰¾åˆ°ç›¸å…³é£ä¹¦æ–‡æ¡£"
            
            # ã€å…³é”®ã€‘äºŒæ¬¡æ¸…æ´—ä¸è¯„åˆ†
            official_docs = []  # å®˜æ–¹å¸®åŠ©ä¸­å¿ƒæ–‡æ¡£
            other_feishu = []   # å…¶ä»–é£ä¹¦é¡µé¢
            
            for page in webpages:
                url_str = page.get('url', '')
                
                if self._is_feishu_official_doc(url_str):
                    _, score = self._score_feishu_result(page, query)
                    official_docs.append((page, score))
                elif 'feishu.cn' in url_str.lower() or 'larksuite.com' in url_str.lower():
                    _, score = self._score_feishu_result(page, query)
                    other_feishu.append((page, score))
            
            # æŒ‰è¯„åˆ†æ’åº
            official_docs.sort(key=lambda x: -x[1])
            other_feishu.sort(key=lambda x: -x[1])
            
            if not official_docs and not other_feishu:
                return "æœªæ‰¾åˆ°é£ä¹¦å®˜æ–¹æ–‡æ¡£"
            
            # ä¼˜å…ˆè¿”å›å®˜æ–¹æ–‡æ¡£ï¼Œä¸è¶³åˆ™è¡¥å……å…¶ä»–é£ä¹¦é¡µé¢
            results_to_show = official_docs[:count]
            if len(results_to_show) < count and other_feishu:
                results_to_show.extend(other_feishu[:count - len(results_to_show)])
            
            # æ ¼å¼åŒ–è¾“å‡º
            formatted_results = ""
            if official_docs:
                formatted_results += f"âœ“ æ‰¾åˆ° {len(official_docs)} ä¸ªé£ä¹¦å®˜æ–¹å¸®åŠ©æ–‡æ¡£\n"
            formatted_results += "\n"
            
            for idx, (page, score) in enumerate(results_to_show, start=1):
                is_official = self._is_feishu_official_doc(page['url'])
                tag = "â˜… å®˜æ–¹æ–‡æ¡£" if is_official else "é£ä¹¦ç›¸å…³"
                
                formatted_results += (
                    f"[{idx}] {tag}\n"
                    f"æ ‡é¢˜: {page.get('name', 'æ— æ ‡é¢˜')}\n"
                    f"é“¾æ¥: {page.get('url', '')}\n"
                    f"è¯´æ˜: {page.get('summary', page.get('snippet', 'æ— æ‘˜è¦'))}\n"
                    f"{'-'*60}\n\n"
                )
            
            return formatted_results.strip()
            
        except Exception as e:
            return f"æœç´¢è¿‡ç¨‹å‡ºé”™: {str(e)}"
    
    async def _bocha_feishu_search(self, query: str, count: int = 5) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ Bocha API æœç´¢é£ä¹¦å®˜æ–¹æ–‡æ¡£ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            count: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        # ã€å…³é”®ã€‘å¼ºåˆ¶é™å®šé£ä¹¦å®˜æ–¹åŸŸå + å¸®åŠ©ä¸­å¿ƒ
        search_query = f"site:feishu.cn å¸®åŠ©ä¸­å¿ƒ {query}"
        
        if self.verbose:
            print(f"[è”ç½‘æœç´¢] æœç´¢å…³é”®è¯: {search_query}")
        
        if not self.bocha_api_key:
            print("[è”ç½‘æœç´¢] æœªé…ç½® Bocha API Keyï¼Œè·³è¿‡è”ç½‘æœç´¢")
            return []
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"{self.bocha_base_url}/web-search",
                    headers={
                        "Authorization": f"Bearer {self.bocha_api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "query": search_query,
                        "summary": True,
                        "count": 30,
                        "freshness": "noLimit"
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    webpages = data.get("data", {}).get("webPages", {}).get("value", [])
                    
                    # äºŒæ¬¡æ¸…æ´—ä¸è¯„åˆ†
                    official_docs = []
                    other_feishu = []
                    
                    for page in webpages:
                        url_str = page.get('url', '')
                        if self._is_feishu_official_doc(url_str):
                            _, score = self._score_feishu_result(page, query)
                            official_docs.append((page, score))
                        elif 'feishu.cn' in url_str.lower() or 'larksuite.com' in url_str.lower():
                            _, score = self._score_feishu_result(page, query)
                            other_feishu.append((page, score))
                    
                    # æ’åº
                    official_docs.sort(key=lambda x: -x[1])
                    other_feishu.sort(key=lambda x: -x[1])
                    
                    # åˆå¹¶ç»“æœ
                    results = [p for p, _ in official_docs[:count]]
                    if len(results) < count:
                        results.extend([p for p, _ in other_feishu[:count - len(results)]])
                    
                    print(f"[è”ç½‘æœç´¢] è·å–åˆ° {len(results)} æ¡æœ‰æ•ˆç»“æœï¼ˆå…¶ä¸­å®˜æ–¹æ–‡æ¡£ {len(official_docs)} æ¡ï¼‰")
                    return results
                else:
                    print(f"[è”ç½‘æœç´¢] API è¿”å›é”™è¯¯: {response.status_code}")
                    return []
                    
        except Exception as e:
            print(f"[è”ç½‘æœç´¢] æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    async def search_cache(self, question: str, top_k: int = 3) -> Optional[Dict[str, Any]]:
        """
        åœ¨ PostgreSQL å‘é‡åº“ä¸­æœç´¢ç›¸ä¼¼é—®é¢˜
        
        Args:
            question: æŸ¥è¯¢é—®é¢˜
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            å¦‚æœæ‰¾åˆ°ç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼çš„ç»“æœï¼Œè¿”å›æœ€ç›¸ä¼¼çš„ç»“æœï¼›å¦åˆ™è¿”å› None
        """
        if not self.enable_cache:
            return None
            
        try:
            crud = self._get_crud()
            results = crud.search_by_question(
                query_text=question,
                top_k=top_k,
                threshold=self.similarity_threshold
            )
            
            if results and len(results) > 0:
                best_match = results[0]
                if self.verbose:
                    print(f"[ç¼“å­˜æŸ¥è¯¢] æ‰¾åˆ°ç›¸ä¼¼é—®é¢˜ï¼Œç›¸ä¼¼åº¦: {best_match.get('similarity', 0):.4f}")
                    print(f"[ç¼“å­˜æŸ¥è¯¢] åŸé—®é¢˜: {best_match.get('question_text', '')[:100]}...")
                return best_match
            else:
                if self.verbose:
                    print(f"[ç¼“å­˜æŸ¥è¯¢] æœªæ‰¾åˆ°ç›¸ä¼¼åº¦ >= {self.similarity_threshold} çš„ç¼“å­˜ç»“æœ")
                return None
                
        except Exception as e:
            print(f"[ç¼“å­˜æŸ¥è¯¢] æœç´¢å¤±è´¥: {str(e)}")
            return None
    
    async def save_to_cache(self, question: str, answer: str) -> bool:
        """
        å°†é—®ç­”å¯¹å­˜å…¥ PostgreSQL å‘é‡åº“
        
        Args:
            question: é—®é¢˜æ–‡æœ¬ï¼ˆåŸå§‹ä»»åŠ¡ï¼‰
            answer: ç­”æ¡ˆæ–‡æœ¬ï¼ˆå¢å¼ºåçš„ä»»åŠ¡æè¿°ï¼‰
            
        Returns:
            æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        if not self.enable_cache:
            return False
            
        try:
            crud = self._get_crud()
            knowledge = crud.create(
                question_text=question,
                answer_text=answer
            )
            if self.verbose:
                print(f"[ç¼“å­˜ä¿å­˜] æˆåŠŸä¿å­˜çŸ¥è¯†æ¡ç›®ï¼ŒID: {knowledge.id}")
            return True
        except Exception as e:
            print(f"[ç¼“å­˜ä¿å­˜] ä¿å­˜å¤±è´¥: {str(e)}")
            return False
    
    def _execute_agent_search(self, user_question: str) -> Tuple[str, List[str]]:
        """
        æ‰§è¡Œ Agent æœç´¢ï¼ˆReAct æ¨¡å¼ï¼‰
        
        ã€æ ¸å¿ƒæ”¹è¿›ã€‘ä½¿ç”¨ LangChain Agent è®©æ¨¡å‹è‡ªä¸»å†³å®šæœç´¢ç­–ç•¥ï¼Œ
        å¯ä»¥å¤šæ¬¡æœç´¢å¹¶æ•´åˆç»“æœã€‚
        
        Args:
            user_question: ç”¨æˆ·åŸå§‹é—®é¢˜/ä»»åŠ¡
            
        Returns:
            (å¢å¼ºåçš„ä»»åŠ¡æè¿°, å¼•ç”¨çš„URLåˆ—è¡¨)
        """
        # åˆå§‹åŒ– Agent
        agent = initialize_agent(
            tools=[self.feishu_search_tool],
            llm=self.llm,
            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
            verbose=self.verbose,
            max_iterations=self.max_agent_iterations,
            early_stopping_method="generate",
            handle_parsing_errors=True
        )
        
        # ã€å…³é”®ã€‘ä¸“ä¸šåŒ–æç¤ºè¯ - é£ä¹¦ PC ç«¯ UI è‡ªåŠ¨åŒ–æµ‹è¯•å·¥ç¨‹å¸ˆè§’è‰²
        prompt = f"""
ä½ ç°åœ¨çš„èº«ä»½æ˜¯ï¼š**é£ä¹¦PCæ¡Œé¢ç«¯ï¼ˆWindows/macOSï¼‰UIè‡ªåŠ¨åŒ–æµ‹è¯•å·¥ç¨‹å¸ˆ**ã€‚
ä½ çš„è¿è¡Œç¯å¢ƒæ˜¯ï¼š**å•å°ç”µè„‘ã€å•ä¸ªæµ‹è¯•è´¦å·**ã€‚

ä»»åŠ¡ï¼šæ ¹æ®ç”¨æˆ·æä¾›çš„æµ‹è¯•ç‚¹ï¼Œåˆ©ç”¨ FeishuDocSearch æœç´¢é£ä¹¦å®˜æ–¹å¸®åŠ©æ–‡æ¡£ï¼Œå°†å…¶è¡¥å…¨ä¸ºä¸€æ¡**ä»…å½“å‰PCå®¢æˆ·ç«¯ç”¨æˆ·å¯è§å¯æ“ä½œ**çš„æµ‹è¯•æ­¥éª¤æè¿°ã€‚

ç”¨æˆ·åŸå§‹æµ‹è¯•ç‚¹: "{user_question}"

è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹ã€å››å¤§åŸåˆ™ã€‘ï¼š

1. **å•äººè§†è§’åŸåˆ™ï¼ˆæ ¸å¿ƒï¼‰**
   - åœºæ™¯ä¸­åªèƒ½å­˜åœ¨"å½“å‰ç”¨æˆ·"ä¸€ä¸ªæ“ä½œè€…ã€‚
   - **ä¸¥ç¦**æè¿°æ¥æ”¶æ–¹çš„ä¸»åŠ¨è¡Œä¸ºï¼ˆå¦‚"æ¥æ”¶æ–¹ç‚¹å‡»"ã€"ç­‰å¾…å¯¹æ–¹å›å¤"ï¼‰ã€‚
   - æ‰€æœ‰éªŒè¯ç‚¹å¿…é¡»æ˜¯å½“å‰ç”¨æˆ·ç•Œé¢ä¸Šå¯è§çš„å†…å®¹ã€‚

2. **åŠ¨æ€ç›®æ ‡ç­–ç•¥**
   - **ä¼˜å…ˆæå–**ï¼šå¦‚æœç”¨æˆ·åŸå§‹æµ‹è¯•ç‚¹ä¸­æŒ‡å®šäº†æ¥æ”¶å¯¹è±¡ï¼ˆå¦‚"è½¬å‘ç»™å¼ ä¸‰"ã€"åˆ†äº«åˆ°æµ‹è¯•ç¾¤"ï¼‰ï¼Œè¯·åœ¨æ­¥éª¤ä¸­æ˜ç¡®æè¿°"åœ¨è”ç³»äººé€‰æ‹©å™¨ä¸­æœç´¢å¹¶é€‰ä¸­**'å¼ ä¸‰'**"ã€‚
   - **é»˜è®¤å…œåº•**ï¼šå¦‚æœåŸå§‹æµ‹è¯•ç‚¹æœªæŒ‡å®šå¯¹è±¡ï¼ˆä»…è¯´"è½¬å‘æ¶ˆæ¯"ï¼‰ï¼Œåˆ™é»˜è®¤æ“ä½œå¯¹è±¡ä¸º**"æ–‡ä»¶ä¼ è¾“åŠ©æ‰‹"**æˆ–**"å½“å‰ç”¨æˆ·è‡ªå·±"**ã€‚
   - **å°†äººè§†ä¸ºUIå…ƒç´ **ï¼šé€‰æ‹©è”ç³»äººçš„è¿‡ç¨‹åº”æè¿°ä¸ºUIæ“ä½œï¼ˆæœç´¢ã€ç‚¹å‡»å‹¾é€‰ã€ç¡®è®¤ï¼‰ã€‚

3. **PCç«¯äº¤äº’è§„èŒƒ**
   - ä½¿ç”¨PCæ¡Œé¢ç«¯æœ¯è¯­ï¼šå¦‚"é¼ æ ‡å·¦é”®/å³é”®ç‚¹å‡»"ã€"å¤šé€‰æ¶ˆæ¯"ã€"åˆå¹¶è½¬å‘"ã€"ä¾§è¾¹æ "ã€"æ–°çª—å£æ‰“å¼€"ã€‚
   - æè¿°å®Œæ•´é“¾è·¯ï¼šé€‰ä¸­æ¶ˆæ¯ -> ç‚¹å‡»è½¬å‘ -> **æœç´¢å¹¶é€‰ä¸­ç›®æ ‡ï¼ˆæŒ‡å®šç”¨æˆ·æˆ–é»˜è®¤ï¼‰** -> ç‚¹å‡»å‘é€ -> **è‡ªæˆ‘éªŒè¯**ã€‚
   - æ˜ç¡®æ“ä½œå…¥å£ï¼šå¦‚"åœ¨èŠå¤©ç•Œé¢å³ä¸Šè§’ç‚¹å‡»..."ã€"åœ¨ä¾§è¾¹æ æ–‡æ¡£åˆ—è¡¨ä¸­..."

4. **è‡ªæˆ‘éªŒè¯åŸåˆ™**
   - éªŒè¯ç‚¹å¿…é¡»åœ¨å½“å‰ç”¨æˆ·çš„ç•Œé¢ä¸Šã€‚
   - ç¤ºä¾‹ï¼š"å‘é€åï¼Œå½“å‰ç”¨æˆ·åœ¨ä¼šè¯çª—å£ä¸­ç‚¹å‡»åˆšæ‰å‘é€çš„ã€åˆå¹¶è½¬å‘ã€‘å¡ç‰‡ï¼ŒéªŒè¯èƒ½å¦å±•å¼€è¯¦æƒ…å¹¶æ‰“å¼€å…¶ä¸­çš„äº‘æ–‡æ¡£"ã€‚
   - ä¸ä¾èµ–å¤–éƒ¨åé¦ˆï¼Œæ‰€æœ‰éªŒè¯éƒ½æ˜¯å¯¹UIå…ƒç´ çŠ¶æ€çš„æ£€æŸ¥ã€‚

---
**æ‰§è¡Œæ­¥éª¤**:
1. Search: ä½¿ç”¨ FeishuDocSearch æœç´¢åŠŸèƒ½åœ¨PCç«¯çš„å…¥å£ï¼ˆå¦‚ï¼šå³é”®èœå•ä¸­çš„è½¬å‘ï¼‰å’ŒUIè¡¨ç°ã€‚å¦‚æœç¬¬ä¸€æ¬¡æœç´¢ç»“æœä¸å¤Ÿå®Œæ•´ï¼Œå¯ä»¥è°ƒæ•´å…³é”®è¯å†æ¬¡æœç´¢ã€‚
2. Transform: å°†"å‘ç»™æŸäºº"è½¬æ¢ä¸º"UIä¸Šé€‰æ‹©æŸä¸ªè”ç³»äºº"ã€‚
3. Write: è¾“å‡ºç¬¦åˆå•æœºæ“ä½œé€»è¾‘çš„è¯¦ç»†æ­¥éª¤ã€‚

---

ç°åœ¨ï¼Œè¯·å¼€å§‹æ‰§è¡Œä»»åŠ¡ã€‚
è¯·è®°ä½ï¼š**é€‰æ‹©è”ç³»äººåªæ˜¯ä¸€ä¸ªUIç‚¹å‡»åŠ¨ä½œï¼Œä¸éœ€è¦å¯¹æ–¹çœŸçš„åœ¨çº¿æˆ–å›åº”**ã€‚
è¯·åªè¾“å‡ºæœ€ç»ˆçš„æµ‹è¯•æ­¥éª¤æè¿°ï¼Œä¸è¦è¾“å‡ºæ€è€ƒè¿‡ç¨‹ã€‚
"""
        
        source_urls = []
        try:
            result = agent.run(prompt)
            
            # æå– URL
            url_pattern = r'https?://[^\s\[\]`"]+'
            source_urls = list(set(re.findall(url_pattern, result)))
            
            # æ¸…ç†ç»“æœ
            if "Final Answer:" in result:
                result = result.split("Final Answer:")[-1].strip()
            
            # ç§»é™¤ URL
            result = re.sub(r'https?://\S+', '', result)
            # ç§»é™¤æ ‡è®°
            result = re.sub(r'\[å®˜æ–¹æ–‡æ¡£\]|\[é£ä¹¦ç›¸å…³\]|\[[0-9]+\]|å¼•ç”¨\s*\d+|â˜…', '', result)
            # æ¸…ç†å¤šä½™ç©ºç™½
            result = re.sub(r'\n+', '\n', result)
            result = re.sub(r'\s+', ' ', result).strip()
            # ç§»é™¤ Agent è¾“å‡ºæ ¼å¼
            result = re.sub(r'(Thought:|Action:|Action Input:|Observation:).*', '', result, flags=re.IGNORECASE)
            
            return result, source_urls
            
        except Exception as e:
            error_msg = str(e)
            
            # å°è¯•ä»é”™è¯¯ä¿¡æ¯ä¸­æå–æœ‰æ•ˆè¾“å‡º
            if "Could not parse LLM output:" in error_msg:
                matches = re.findall(r'`([^`]+)`', error_msg)
                if matches:
                    result = max(matches, key=len)
                    
                    if len(result) < 50:
                        if "Could not parse LLM output:" in error_msg:
                            output_part = error_msg.split("Could not parse LLM output:")[-1]
                            output_part = output_part.strip(' :`"')
                            if len(output_part) > len(result):
                                result = output_part
                    
                    # æ¸…ç†
                    result = re.sub(r'^(Thought:|Action:|Action Input:|Observation:|Final Answer:)\s*', '', result, flags=re.IGNORECASE)
                    result = re.sub(r'(Thought:|Action:|Action Input:|Observation:).*$', '', result, flags=re.IGNORECASE)
                    result = re.sub(r'https?://\S+', '', result)
                    result = re.sub(r'\[å®˜æ–¹æ–‡æ¡£\]|\[é£ä¹¦ç›¸å…³\]|\[[0-9]+\]|å¼•ç”¨\s*\d+|â˜…', '', result)
                    result = re.sub(r'\s*[`"\'\n]+\s*', ' ', result)
                    result = re.sub(r'\s+', ' ', result).strip()
                    
                    if 30 <= len(result) <= 2000:
                        return result, source_urls
            
            return f"ä»»åŠ¡å¢å¼ºå¤±è´¥ã€‚åŸå§‹ä»»åŠ¡: {user_question}ã€‚å»ºè®®: 1) ç®€åŒ–ä»»åŠ¡æè¿° 2) æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚é”™è¯¯: {error_msg[:100]}", []
    
    async def generate_enhanced_task_with_llm(
        self,
        original_task: str,
        search_results: List[Dict[str, Any]],
        background_knowledge: str = ""
    ) -> str:
        """
        ä½¿ç”¨ LLM ç›´æ¥æ•´åˆæœç´¢ç»“æœç”Ÿæˆå¢å¼ºä»»åŠ¡ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼Œå½“ Agent æ¨¡å¼ä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰
        
        Args:
            original_task: åŸå§‹ä»»åŠ¡æè¿°
            search_results: è”ç½‘æœç´¢ç»“æœ
            background_knowledge: èƒŒæ™¯çŸ¥è¯†
            
        Returns:
            å¢å¼ºåçš„ä»»åŠ¡æè¿°
        """
        # æ•´ç†æœç´¢ç»“æœ
        search_context = ""
        if search_results:
            search_snippets = []
            for i, result in enumerate(search_results[:5], 1):
                title = result.get("name", "")
                snippet = result.get("summary", result.get("snippet", ""))
                url = result.get("url", "")
                is_official = "â˜… å®˜æ–¹æ–‡æ¡£" if self._is_feishu_official_doc(url) else ""
                search_snippets.append(f"{i}. {is_official} {title}\n   {snippet}")
            search_context = "\n".join(search_snippets)
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªé£ä¹¦PCæ¡Œé¢ç«¯UIè‡ªåŠ¨åŒ–æµ‹è¯•å·¥ç¨‹å¸ˆã€‚ä½ çš„èŒè´£æ˜¯ï¼š
1. æ ¹æ®æä¾›çš„é£ä¹¦å®˜æ–¹æ–‡æ¡£æœç´¢ç»“æœï¼Œè¡¥å…¨å’Œä¼˜åŒ–ç”¨æˆ·çš„æµ‹è¯•ä»»åŠ¡æè¿°
2. å°†æ¨¡ç³Šçš„æ¦‚å¿µæ›¿æ¢ä¸ºå…·ä½“ã€æ˜ç¡®çš„PCç«¯UIæ“ä½œæ­¥éª¤
3. éµå¾ª"å•äººè§†è§’åŸåˆ™"ï¼šåªæè¿°å½“å‰ç”¨æˆ·çš„æ“ä½œï¼Œä¸æè¿°æ¥æ”¶æ–¹è¡Œä¸º
4. éµå¾ª"è‡ªæˆ‘éªŒè¯åŸåˆ™"ï¼šéªŒè¯ç‚¹å¿…é¡»åœ¨å½“å‰ç”¨æˆ·ç•Œé¢ä¸Šå¯è§

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- è¾“å‡ºæ¸…æ™°ã€å¯æ‰§è¡Œçš„æµ‹è¯•æ­¥éª¤
- ä½¿ç”¨PCç«¯æœ¯è¯­ï¼ˆé¼ æ ‡ç‚¹å‡»ã€å³é”®èœå•ã€ä¾§è¾¹æ ç­‰ï¼‰
- ä¸æ·»åŠ é¢å¤–çš„è§£é‡Šï¼Œåªè¾“å‡ºæ­¥éª¤"""

        context_parts = []
        if search_context:
            context_parts.append(f"ã€é£ä¹¦å®˜æ–¹æ–‡æ¡£æœç´¢ç»“æœã€‘\n{search_context}")
        if background_knowledge:
            context_parts.append(f"ã€èƒŒæ™¯çŸ¥è¯†ã€‘\n{background_knowledge}")
        
        context = "\n\n".join(context_parts) if context_parts else "æ— é¢å¤–ä¸Šä¸‹æ–‡"
        
        user_prompt = f"""è¯·å°†ä»¥ä¸‹æµ‹è¯•ç‚¹è¡¥å…¨ä¸ºè¯¦ç»†çš„PCç«¯UIæ“ä½œæ­¥éª¤ï¼š

ã€åŸå§‹æµ‹è¯•ç‚¹ã€‘
{original_task}

ã€å‚è€ƒä¿¡æ¯ã€‘
{context}

ã€è¡¥å…¨åçš„æµ‹è¯•æ­¥éª¤ã€‘"""

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        try:
            response = await self.llm.ainvoke(messages)
            enhanced_task = response.content.strip()
            return enhanced_task
        except Exception as e:
            print(f"[LLMç”Ÿæˆ] ç”Ÿæˆå¤±è´¥: {str(e)}")
            return original_task
    
    async def enhance(self, task: str, force_refresh: bool = False) -> Dict[str, Any]:
        """
        å¢å¼ºä»»åŠ¡æè¿°çš„ä¸»å…¥å£
        
        æµç¨‹ï¼š
        1. æœç´¢ç¼“å­˜ï¼ˆPostgreSQL å‘é‡åº“ï¼‰
        2. ç¼“å­˜å‘½ä¸­ -> ç›´æ¥è¿”å›ç¼“å­˜çš„ç­”æ¡ˆ
        3. ç¼“å­˜æœªå‘½ä¸­ -> Agent æœç´¢ï¼ˆReAct æ¨¡å¼ï¼‰-> å­˜å…¥ç¼“å­˜ -> è¿”å›ç»“æœ
        
        Args:
            task: åŸå§‹ä»»åŠ¡æè¿°
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ï¼ˆè·³è¿‡ç¼“å­˜ï¼‰
            
        Returns:
            åŒ…å«å¢å¼ºç»“æœçš„å­—å…¸ï¼š
            - enhanced_task: å¢å¼ºåçš„ä»»åŠ¡æè¿°
            - cache_hit: æ˜¯å¦å‘½ä¸­ç¼“å­˜
            - search_performed: æ˜¯å¦æ‰§è¡Œäº†è”ç½‘æœç´¢
            - source: ç»“æœæ¥æº ("cache" / "agent" / "original")
            - source_urls: å¼•ç”¨çš„URLåˆ—è¡¨
        """
        if self.verbose:
            print(f"\n{'='*60}")
            print(f"ğŸ“ ä»»åŠ¡å¢å¼ºå¼€å§‹: {task[:100]}...")
            print(f"{'='*60}")
        
        result = {
            "original_task": task,
            "enhanced_task": task,
            "cache_hit": False,
            "search_performed": False,
            "source": "original",
            "source_urls": []
        }
        
        # Step 1: æœç´¢ç¼“å­˜ï¼ˆé™¤éå¼ºåˆ¶åˆ·æ–°ï¼‰
        if not force_refresh:
            if self.verbose:
                print("\n[Step 1] æœç´¢å‘é‡ç¼“å­˜...")
            cache_result = await self.search_cache(task)
            
            if cache_result:
                if self.verbose:
                    print(f"å‘½ä¸­ç¼“å­˜ï¼ˆç›¸ä¼¼åº¦: {cache_result.get('similarity', 0):.4f}ï¼‰")
                result["enhanced_task"] = cache_result.get("answer_text", task)
                result["cache_hit"] = True
                result["source"] = "cache"
                result["cache_similarity"] = cache_result.get("similarity", 0)
                return result
        else:
            if self.verbose:
                print("âš¡ å¼ºåˆ¶åˆ·æ–°æ¨¡å¼ï¼Œè·³è¿‡ç¼“å­˜")
        
        # Step 2: ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œ Agent æœç´¢
        if self.enable_web_search:
            if self.verbose:
                print("\n[Step 2] å¯åŠ¨ Agent æœç´¢ï¼ˆReAct æ¨¡å¼ï¼‰...")
            
            # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥çš„ Agent
            enhanced_task, source_urls = await asyncio.to_thread(
                self._execute_agent_search, task
            )
            
            result["enhanced_task"] = enhanced_task
            result["search_performed"] = True
            result["source"] = "agent"
            result["source_urls"] = source_urls
        else:
            if self.verbose:
                print("\nâš ï¸ è”ç½‘æœç´¢å·²ç¦ç”¨ï¼Œä½¿ç”¨åŸå§‹ä»»åŠ¡")
        
        # Step 3: å°†ç»“æœå­˜å…¥ç¼“å­˜
        if result["enhanced_task"] != task:
            if self.verbose:
                print("\n[Step 3] ä¿å­˜åˆ°å‘é‡ç¼“å­˜...")
            await self.save_to_cache(task, result["enhanced_task"])
        
        if self.verbose:
            print(f"\nâœ“ ä»»åŠ¡å¢å¼ºå®Œæˆ")
            print(f"{'='*60}\n")
        
        return result


class FeishuEnhancerConfig:
    """é£ä¹¦å¢å¼ºå™¨é…ç½®ç±»"""
    
    def __init__(self):
        # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶åŠ è½½
        try:
            import config
            self.bocha_api_key = config.config_dict.get("BOCHA_API_KEY", os.getenv("BOCHA_API_KEY", ""))
            self.bocha_base_url = config.config_dict.get("BOCHA_BASE_URL", "https://api.bochaai.com/v1")
            self.similarity_threshold = float(config.config_dict.get("ENHANCE_SIMILARITY_THRESHOLD", "0.85"))
            self.enable_cache = str(config.config_dict.get("ENHANCE_ENABLE_CACHE", "true")).lower() == "true"
            self.enable_web_search = str(config.config_dict.get("ENHANCE_ENABLE_WEB_SEARCH", "true")).lower() == "true"
        except ImportError:
            self.bocha_api_key = os.getenv("BOCHA_API_KEY", "")
            self.bocha_base_url = os.getenv("BOCHA_BASE_URL", "https://api.bochaai.com/v1")
            self.similarity_threshold = float(os.getenv("ENHANCE_SIMILARITY_THRESHOLD", "0.85"))
            self.enable_cache = os.getenv("ENHANCE_ENABLE_CACHE", "true").lower() == "true"
            self.enable_web_search = os.getenv("ENHANCE_ENABLE_WEB_SEARCH", "true").lower() == "true"


# å…¨å±€å¢å¼ºå™¨å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
_enhancer_instance: Optional[FeishuTaskEnhancer] = None


def get_feishu_enhancer(llm: ChatOpenAI) -> FeishuTaskEnhancer:
    """
    è·å–é£ä¹¦å¢å¼ºå™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    
    Args:
        llm: LangChain ChatOpenAI å®ä¾‹
        
    Returns:
        FeishuTaskEnhancer å®ä¾‹
    """
    global _enhancer_instance
    if _enhancer_instance is None:
        config = FeishuEnhancerConfig()
        _enhancer_instance = FeishuTaskEnhancer(
            llm=llm,
            bocha_api_key=config.bocha_api_key,
            bocha_base_url=config.bocha_base_url,
            similarity_threshold=config.similarity_threshold,
            enable_cache=config.enable_cache,
            enable_web_search=config.enable_web_search
        )
    return _enhancer_instance