"""
é£ä¹¦ä»»åŠ¡å¢å¼ºå™¨ - è”ç½‘æœç´¢ + PostgreSQL å‘é‡ç¼“å­˜
"""
import os
import re
import asyncio
import httpx
from typing import Optional, Dict, Any, List, Tuple

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

# å¯¼å…¥ä¸šåŠ¡çŸ¥è¯†åº“ç›¸å…³æ¨¡å—
from business_knowledge.database import get_db
from business_knowledge.crud import BusinessKnowledgeCRUD


class FeishuTaskEnhancer:
    """
    é£ä¹¦ä»»åŠ¡å¢å¼ºå™¨
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. è¯­ä¹‰ç¼“å­˜ï¼ˆPostgreSQL å‘é‡æ•°æ®åº“ï¼‰
    2. æ™ºèƒ½æ£€ç´¢ï¼ˆç›¸ä¼¼åº¦ >= é˜ˆå€¼ç›´æ¥è¿”å›ï¼‰
    3. è”ç½‘æœç´¢ï¼ˆBocha APIï¼Œé™å®šé£ä¹¦å®˜ç½‘ï¼‰
    4. è‡ªåŠ¨æ²‰æ·€ï¼ˆæœç´¢ç»“æœè‡ªåŠ¨å…¥åº“ï¼‰
    """
    
    # é£ä¹¦å®˜æ–¹æ–‡æ¡£ URL æ¨¡å¼ï¼ˆç™½åå•ï¼‰
    FEISHU_OFFICIAL_URL_PATTERNS = [
        'https://www.feishu.cn/hc/zh-CN/articles/',
        'https://www.feishu.cn/hc/en-US/articles/',
        'https://www.feishu.cn/hc/zh-CN/',
        'https://www.feishu.cn/hc/en-US/',
        'https://www.larksuite.com/hc/',
        'https://feishu.cn/hc/',
    ]
    
    def __init__(
        self,
        llm: ChatOpenAI,
        bocha_api_key: Optional[str] = None,
        bocha_base_url: str = "https://api.bochaai.com/v1",
        similarity_threshold: float = 0.85,
        enable_cache: bool = True,
        enable_web_search: bool = True,
        verbose: bool = True,
        max_search_iterations: int = 2
    ):
        """
        åˆå§‹åŒ–å¢å¼ºå™¨
        
        Args:
            llm: LangChain ChatOpenAI å®ä¾‹
            bocha_api_key: Bocha API å¯†é’¥ï¼ˆç”¨äºè”ç½‘æœç´¢ï¼‰
            bocha_base_url: Bocha API åŸºç¡€ URL
            similarity_threshold: ç¼“å­˜å‘½ä¸­çš„ç›¸ä¼¼åº¦é˜ˆå€¼
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜
            enable_web_search: æ˜¯å¦å¯ç”¨è”ç½‘æœç´¢
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
            max_search_iterations: æœ€å¤§æœç´¢è¿­ä»£æ¬¡æ•°
        """
        self.llm = llm
        self.bocha_api_key = bocha_api_key or os.getenv("BOCHA_API_KEY", "")
        self.bocha_base_url = bocha_base_url
        self.similarity_threshold = similarity_threshold
        self.enable_cache = enable_cache
        self.enable_web_search = enable_web_search
        self.verbose = verbose
        self.max_search_iterations = max_search_iterations
        
        if self.verbose:
            print(f"âœ“ FeishuTaskEnhancer åˆå§‹åŒ–å®Œæˆ")
            print(f"  - ç›¸ä¼¼åº¦é˜ˆå€¼: {self.similarity_threshold}")
            print(f"  - ç¼“å­˜å¯ç”¨: {self.enable_cache}")
            print(f"  - è”ç½‘æœç´¢å¯ç”¨: {self.enable_web_search}")
        
    def _get_crud(self) -> BusinessKnowledgeCRUD:
        """è·å–æ•°æ®åº“ CRUD å®ä¾‹"""
        db = next(get_db())
        return BusinessKnowledgeCRUD(db)
    
    def _is_feishu_official_doc(self, url: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦æ˜¯é£ä¹¦å®˜æ–¹æ–‡æ¡£
        
        Args:
            url: ç½‘é¡µ URL
            
        Returns:
            æ˜¯å¦ä¸ºå®˜æ–¹æ–‡æ¡£
        """
        return any(url.startswith(pattern) for pattern in self.FEISHU_OFFICIAL_URL_PATTERNS)
    
    def _score_feishu_result(self, page: Dict, query: str) -> Tuple[bool, int]:
        """
        å¯¹æœç´¢ç»“æœè¿›è¡Œè¯„åˆ†
        
        è¯„åˆ†è§„åˆ™ï¼š
        - å®˜æ–¹æ–‡æ¡£ (feishu.cn/hc/): +100 åˆ†
        - æ ‡é¢˜åŒ…å«æŸ¥è¯¢è¯: +10 åˆ†
        - æ‘˜è¦åŒ…å«æŸ¥è¯¢è¯: +5 åˆ†
        - URL åŒ…å« /hc/: +20 åˆ†
        - æ‘˜è¦é•¿åº¦é€‚ä¸­ (50-500): +3 åˆ†
        
        Args:
            page: æœç´¢ç»“æœé¡µé¢ä¿¡æ¯
            query: åŸå§‹æŸ¥è¯¢
            
        Returns:
            (æ˜¯å¦å®˜æ–¹æ–‡æ¡£, è¯„åˆ†)
        """
        url = page.get('url', '')
        name = page.get('name', '').lower()
        summary = page.get('summary', page.get('snippet', '')).lower()
        query_lower = query.lower()
        
        is_official = self._is_feishu_official_doc(url)
        score = 0
        
        # å®˜æ–¹æ–‡æ¡£åŠ åˆ†
        if is_official:
            score += 100
        
        # æ ‡é¢˜åŒ¹é…
        if query_lower in name:
            score += 10
        
        # æ‘˜è¦åŒ¹é…
        if query_lower in summary:
            score += 5
        
        # å¸®åŠ©ä¸­å¿ƒè·¯å¾„åŠ åˆ†
        if 'feishu.cn/hc' in url or '/hc/' in url:
            score += 20
        
        # æ‘˜è¦é•¿åº¦é€‚ä¸­åŠ åˆ†
        summary_len = len(summary)
        if 50 < summary_len < 500:
            score += 3
        
        return (is_official, score)
    
    def _bocha_feishu_search_sync(self, query: str, count: int = 5) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ Bocha API æœç´¢é£ä¹¦å®˜æ–¹æ–‡æ¡£ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            count: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        # ã€å…³é”®ã€‘å¼ºåˆ¶é™å®šé£ä¹¦å®˜æ–¹åŸŸå + å¸®åŠ©ä¸­å¿ƒ
        search_query = f"site:feishu.cn å¸®åŠ©ä¸­å¿ƒ {query}"
        
        if self.verbose:
            print(f"[è”ç½‘æœç´¢] æœç´¢å…³é”®è¯: {search_query}")
        
        if not self.bocha_api_key:
            print("[è”ç½‘æœç´¢] æœªé…ç½® Bocha API Keyï¼Œè·³è¿‡è”ç½‘æœç´¢")
            return []
        
        import requests
        
        url = f"{self.bocha_base_url}/web-search"
        headers = {
            'Authorization': f'Bearer {self.bocha_api_key}',
            'Content-Type': 'application/json'
        }
        data = {
            "query": search_query,
            "freshness": "noLimit",
            "summary": True,
            "count": 30  # å¤šå–ä¸€äº›ï¼Œç”¨äºç­›é€‰
        }
        
        try:
            response = requests.post(url, headers=headers, json=data, timeout=30)
            
            if response.status_code != 200:
                print(f"[è”ç½‘æœç´¢] API è¿”å›é”™è¯¯: {response.status_code}")
                return []
            
            json_response = response.json()
            if json_response.get("code") != 200 or not json_response.get("data"):
                print(f"[è”ç½‘æœç´¢] API è¿”å›é”™è¯¯: {json_response.get('msg', 'æœªçŸ¥é”™è¯¯')}")
                return []
            
            webpages = json_response["data"].get("webPages", {}).get("value", [])
            
            if not webpages:
                return []
            
            # ã€å…³é”®ã€‘äºŒæ¬¡æ¸…æ´—ä¸è¯„åˆ†
            official_docs = []  # å®˜æ–¹å¸®åŠ©ä¸­å¿ƒæ–‡æ¡£
            other_feishu = []   # å…¶ä»–é£ä¹¦é¡µé¢
            
            for page in webpages:
                url_str = page.get('url', '')
                
                if self._is_feishu_official_doc(url_str):
                    _, score = self._score_feishu_result(page, query)
                    official_docs.append((page, score))
                elif 'feishu.cn' in url_str.lower() or 'larksuite.com' in url_str.lower():
                    _, score = self._score_feishu_result(page, query)
                    other_feishu.append((page, score))
            
            # æŒ‰è¯„åˆ†æ’åº
            official_docs.sort(key=lambda x: -x[1])
            other_feishu.sort(key=lambda x: -x[1])
            
            # ä¼˜å…ˆè¿”å›å®˜æ–¹æ–‡æ¡£ï¼Œä¸è¶³åˆ™è¡¥å……å…¶ä»–é£ä¹¦é¡µé¢
            results = [p for p, _ in official_docs[:count]]
            if len(results) < count and other_feishu:
                results.extend([p for p, _ in other_feishu[:count - len(results)]])
            
            print(f"[è”ç½‘æœç´¢] è·å–åˆ° {len(results)} æ¡æœ‰æ•ˆç»“æœï¼ˆå…¶ä¸­å®˜æ–¹æ–‡æ¡£ {len(official_docs)} æ¡ï¼‰")
            return results
            
        except Exception as e:
            print(f"[è”ç½‘æœç´¢] æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    async def _bocha_feishu_search(self, query: str, count: int = 5) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ Bocha API æœç´¢é£ä¹¦å®˜æ–¹æ–‡æ¡£ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            count: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        # ã€å…³é”®ã€‘å¼ºåˆ¶é™å®šé£ä¹¦å®˜æ–¹åŸŸå + å¸®åŠ©ä¸­å¿ƒ
        search_query = f"site:feishu.cn å¸®åŠ©ä¸­å¿ƒ {query}"
        
        if self.verbose:
            print(f"[è”ç½‘æœç´¢] æœç´¢å…³é”®è¯: {search_query}")
        
        if not self.bocha_api_key:
            print("[è”ç½‘æœç´¢] æœªé…ç½® Bocha API Keyï¼Œè·³è¿‡è”ç½‘æœç´¢")
            return []
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"{self.bocha_base_url}/web-search",
                    headers={
                        "Authorization": f"Bearer {self.bocha_api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "query": search_query,
                        "summary": True,
                        "count": 30,
                        "freshness": "noLimit"
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    webpages = data.get("data", {}).get("webPages", {}).get("value", [])
                    
                    # äºŒæ¬¡æ¸…æ´—ä¸è¯„åˆ†
                    official_docs = []
                    other_feishu = []
                    
                    for page in webpages:
                        url_str = page.get('url', '')
                        if self._is_feishu_official_doc(url_str):
                            _, score = self._score_feishu_result(page, query)
                            official_docs.append((page, score))
                        elif 'feishu.cn' in url_str.lower() or 'larksuite.com' in url_str.lower():
                            _, score = self._score_feishu_result(page, query)
                            other_feishu.append((page, score))
                    
                    # æ’åº
                    official_docs.sort(key=lambda x: -x[1])
                    other_feishu.sort(key=lambda x: -x[1])
                    
                    # åˆå¹¶ç»“æœ
                    results = [p for p, _ in official_docs[:count]]
                    if len(results) < count:
                        results.extend([p for p, _ in other_feishu[:count - len(results)]])
                    
                    print(f"[è”ç½‘æœç´¢] è·å–åˆ° {len(results)} æ¡æœ‰æ•ˆç»“æœï¼ˆå…¶ä¸­å®˜æ–¹æ–‡æ¡£ {len(official_docs)} æ¡ï¼‰")
                    return results
                else:
                    print(f"[è”ç½‘æœç´¢] API è¿”å›é”™è¯¯: {response.status_code}")
                    return []
                    
        except Exception as e:
            print(f"[è”ç½‘æœç´¢] æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    async def search_cache(self, question: str, top_k: int = 3) -> Optional[Dict[str, Any]]:
        """
        åœ¨ PostgreSQL å‘é‡åº“ä¸­æœç´¢ç›¸ä¼¼é—®é¢˜
        
        Args:
            question: æŸ¥è¯¢é—®é¢˜
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            å¦‚æœæ‰¾åˆ°ç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼çš„ç»“æœï¼Œè¿”å›æœ€ç›¸ä¼¼çš„ç»“æœï¼›å¦åˆ™è¿”å› None
        """
        if not self.enable_cache:
            return None
            
        try:
            crud = self._get_crud()
            results = crud.search_by_question(
                query_text=question,
                top_k=top_k,
                threshold=self.similarity_threshold
            )
            
            if results and len(results) > 0:
                best_match = results[0]
                if self.verbose:
                    print(f"[ç¼“å­˜æŸ¥è¯¢] æ‰¾åˆ°ç›¸ä¼¼é—®é¢˜ï¼Œç›¸ä¼¼åº¦: {best_match.get('similarity', 0):.4f}")
                    print(f"[ç¼“å­˜æŸ¥è¯¢] åŸé—®é¢˜: {best_match.get('question_text', '')[:100]}...")
                return best_match
            else:
                if self.verbose:
                    print(f"[ç¼“å­˜æŸ¥è¯¢] æœªæ‰¾åˆ°ç›¸ä¼¼åº¦ >= {self.similarity_threshold} çš„ç¼“å­˜ç»“æœ")
                return None
                
        except Exception as e:
            print(f"[ç¼“å­˜æŸ¥è¯¢] æœç´¢å¤±è´¥: {str(e)}")
            return None
    
    async def save_to_cache(self, question: str, answer: str) -> bool:
        """
        å°†é—®ç­”å¯¹å­˜å…¥ PostgreSQL å‘é‡åº“
        
        Args:
            question: é—®é¢˜æ–‡æœ¬ï¼ˆåŸå§‹ä»»åŠ¡ï¼‰
            answer: ç­”æ¡ˆæ–‡æœ¬ï¼ˆå¢å¼ºåçš„ä»»åŠ¡æè¿°ï¼‰
            
        Returns:
            æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        if not self.enable_cache:
            return False
            
        try:
            crud = self._get_crud()
            knowledge = crud.create(
                question_text=question,
                answer_text=answer
            )
            if self.verbose:
                print(f"[ç¼“å­˜ä¿å­˜] æˆåŠŸä¿å­˜çŸ¥è¯†æ¡ç›®ï¼ŒID: {knowledge.id}")
            return True
        except Exception as e:
            print(f"[ç¼“å­˜ä¿å­˜] ä¿å­˜å¤±è´¥: {str(e)}")
            return False
    
    async def _generate_search_keywords(self, task: str) -> List[str]:
        """
        ä½¿ç”¨ LLM ç”Ÿæˆæœç´¢å…³é”®è¯
        
        Args:
            task: åŸå§‹ä»»åŠ¡æè¿°
            
        Returns:
            æœç´¢å…³é”®è¯åˆ—è¡¨
        """
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªé£ä¹¦äº§å“ä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·çš„æµ‹è¯•ä»»åŠ¡ï¼Œç”Ÿæˆç”¨äºæœç´¢é£ä¹¦å¸®åŠ©æ–‡æ¡£çš„å…³é”®è¯ã€‚

è¦æ±‚ï¼š
1. æå–ä»»åŠ¡ä¸­çš„æ ¸å¿ƒåŠŸèƒ½ç‚¹
2. ä½¿ç”¨é£ä¹¦äº§å“æœ¯è¯­
3. è¿”å› 2-3 ä¸ªæœç´¢å…³é”®è¯ï¼Œæ¯è¡Œä¸€ä¸ª
4. åªè¿”å›å…³é”®è¯ï¼Œä¸è¦å…¶ä»–è§£é‡Š"""

        user_prompt = f"ä»»åŠ¡: {task}\n\nè¯·ç”Ÿæˆæœç´¢å…³é”®è¯ï¼š"

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        try:
            response = await self.llm.ainvoke(messages)
            keywords = [kw.strip() for kw in response.content.strip().split('\n') if kw.strip()]
            return keywords[:3]  # æœ€å¤šè¿”å›3ä¸ªå…³é”®è¯
        except Exception as e:
            print(f"[å…³é”®è¯ç”Ÿæˆ] å¤±è´¥: {str(e)}")
            # å›é€€ï¼šç›´æ¥ä½¿ç”¨åŸä»»åŠ¡ä½œä¸ºå…³é”®è¯
            return [task]
    
    async def _generate_enhanced_task(
        self,
        original_task: str,
        search_results: List[Dict[str, Any]]
    ) -> Tuple[str, List[str]]:
        """
        ä½¿ç”¨ LLM æ ¹æ®æœç´¢ç»“æœç”Ÿæˆå¢å¼ºä»»åŠ¡
        
        Args:
            original_task: åŸå§‹ä»»åŠ¡æè¿°
            search_results: è”ç½‘æœç´¢ç»“æœ
            
        Returns:
            (å¢å¼ºåçš„ä»»åŠ¡æè¿°, å¼•ç”¨çš„URLåˆ—è¡¨)
        """
        # æ•´ç†æœç´¢ç»“æœ
        search_context = ""
        source_urls = []
        
        if search_results:
            search_snippets = []
            for i, result in enumerate(search_results[:5], 1):
                title = result.get("name", "")
                snippet = result.get("summary", result.get("snippet", ""))
                url = result.get("url", "")
                is_official = "â˜… å®˜æ–¹æ–‡æ¡£" if self._is_feishu_official_doc(url) else ""
                search_snippets.append(f"{i}. {is_official} {title}\n   æ‘˜è¦: {snippet}\n   é“¾æ¥: {url}")
                source_urls.append(url)
            search_context = "\n\n".join(search_snippets)
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªé£ä¹¦PCæ¡Œé¢ç«¯UIè‡ªåŠ¨åŒ–æµ‹è¯•å·¥ç¨‹å¸ˆã€‚ä½ çš„èŒè´£æ˜¯ï¼š
1. æ ¹æ®æä¾›çš„é£ä¹¦å®˜æ–¹æ–‡æ¡£æœç´¢ç»“æœï¼Œè¡¥å…¨å’Œä¼˜åŒ–ç”¨æˆ·çš„æµ‹è¯•ä»»åŠ¡æè¿°
2. å°†æ¨¡ç³Šçš„æ¦‚å¿µæ›¿æ¢ä¸ºå…·ä½“ã€æ˜ç¡®çš„PCç«¯UIæ“ä½œæ­¥éª¤

ã€å››å¤§åŸåˆ™ã€‘ï¼š

1. **å•äººè§†è§’åŸåˆ™ï¼ˆæ ¸å¿ƒï¼‰**
   - åœºæ™¯ä¸­åªèƒ½å­˜åœ¨"å½“å‰ç”¨æˆ·"ä¸€ä¸ªæ“ä½œè€…
   - ä¸¥ç¦æè¿°æ¥æ”¶æ–¹çš„ä¸»åŠ¨è¡Œä¸ºï¼ˆå¦‚"æ¥æ”¶æ–¹ç‚¹å‡»"ã€"ç­‰å¾…å¯¹æ–¹å›å¤"ï¼‰
   - æ‰€æœ‰éªŒè¯ç‚¹å¿…é¡»æ˜¯å½“å‰ç”¨æˆ·ç•Œé¢ä¸Šå¯è§çš„å†…å®¹

2. **åŠ¨æ€ç›®æ ‡ç­–ç•¥**
   - å¦‚æœç”¨æˆ·åŸå§‹æµ‹è¯•ç‚¹ä¸­æŒ‡å®šäº†æ¥æ”¶å¯¹è±¡ï¼Œåœ¨æ­¥éª¤ä¸­æ˜ç¡®æè¿°UIé€‰æ‹©æ“ä½œ
   - å¦‚æœæœªæŒ‡å®šå¯¹è±¡ï¼Œé»˜è®¤æ“ä½œå¯¹è±¡ä¸º"æ–‡ä»¶ä¼ è¾“åŠ©æ‰‹"æˆ–"å½“å‰ç”¨æˆ·è‡ªå·±"
   - å°†äººè§†ä¸ºUIå…ƒç´ ï¼šé€‰æ‹©è”ç³»äººçš„è¿‡ç¨‹åº”æè¿°ä¸ºUIæ“ä½œ

3. **PCç«¯äº¤äº’è§„èŒƒ**
   - ä½¿ç”¨PCæ¡Œé¢ç«¯æœ¯è¯­ï¼šé¼ æ ‡å·¦é”®/å³é”®ç‚¹å‡»ã€å¤šé€‰æ¶ˆæ¯ã€ä¾§è¾¹æ ã€æ–°çª—å£æ‰“å¼€ç­‰
   - æè¿°å®Œæ•´é“¾è·¯ï¼šé€‰ä¸­ -> æ“ä½œ -> éªŒè¯
   - æ˜ç¡®æ“ä½œå…¥å£

4. **è‡ªæˆ‘éªŒè¯åŸåˆ™**
   - éªŒè¯ç‚¹å¿…é¡»åœ¨å½“å‰ç”¨æˆ·çš„ç•Œé¢ä¸Š
   - ä¸ä¾èµ–å¤–éƒ¨åé¦ˆï¼Œæ‰€æœ‰éªŒè¯éƒ½æ˜¯å¯¹UIå…ƒç´ çŠ¶æ€çš„æ£€æŸ¥

è¾“å‡ºè¦æ±‚ï¼š
- è¾“å‡ºæ¸…æ™°ã€å¯æ‰§è¡Œçš„æµ‹è¯•æ­¥éª¤
- ä½¿ç”¨PCç«¯æœ¯è¯­
- ä¸æ·»åŠ é¢å¤–çš„è§£é‡Šï¼Œåªè¾“å‡ºæ­¥éª¤
- ä¸è¦è¾“å‡º URL é“¾æ¥"""

        user_prompt = f"""è¯·å°†ä»¥ä¸‹æµ‹è¯•ç‚¹è¡¥å…¨ä¸ºè¯¦ç»†çš„PCç«¯UIæ“ä½œæ­¥éª¤ï¼š

ã€åŸå§‹æµ‹è¯•ç‚¹ã€‘
{original_task}

ã€é£ä¹¦å®˜æ–¹æ–‡æ¡£å‚è€ƒã€‘
{search_context if search_context else "æ— æœç´¢ç»“æœ"}

ã€è¡¥å…¨åçš„æµ‹è¯•æ­¥éª¤ã€‘"""

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ]
        
        try:
            response = await self.llm.ainvoke(messages)
            enhanced_task = response.content.strip()
            
            # æ¸…ç†ç»“æœä¸­å¯èƒ½æ®‹ç•™çš„ URL
            enhanced_task = re.sub(r'https?://\S+', '', enhanced_task)
            enhanced_task = re.sub(r'\s+', ' ', enhanced_task).strip()
            
            return enhanced_task, source_urls
        except Exception as e:
            print(f"[LLMç”Ÿæˆ] ç”Ÿæˆå¤±è´¥: {str(e)}")
            return original_task, []
    
    async def _execute_search_and_enhance(self, task: str) -> Tuple[str, List[str]]:
        """
        æ‰§è¡Œæœç´¢å’Œå¢å¼ºæµç¨‹
        
        æµç¨‹ï¼š
        1. ç”Ÿæˆæœç´¢å…³é”®è¯
        2. æ‰§è¡Œè”ç½‘æœç´¢
        3. ä½¿ç”¨ LLM ç”Ÿæˆå¢å¼ºä»»åŠ¡
        
        Args:
            task: åŸå§‹ä»»åŠ¡æè¿°
            
        Returns:
            (å¢å¼ºåçš„ä»»åŠ¡æè¿°, å¼•ç”¨çš„URLåˆ—è¡¨)
        """
        all_search_results = []
        
        # Step 1: ç”Ÿæˆæœç´¢å…³é”®è¯
        if self.verbose:
            print("[æœç´¢å¢å¼º] ç”Ÿæˆæœç´¢å…³é”®è¯...")
        keywords = await self._generate_search_keywords(task)
        if self.verbose:
            print(f"[æœç´¢å¢å¼º] å…³é”®è¯: {keywords}")
        
        # Step 2: æ‰§è¡Œæœç´¢
        for keyword in keywords:
            if self.verbose:
                print(f"[æœç´¢å¢å¼º] æœç´¢: {keyword}")
            results = await self._bocha_feishu_search(keyword, count=3)
            all_search_results.extend(results)
        
        # å»é‡ï¼ˆåŸºäº URLï¼‰
        seen_urls = set()
        unique_results = []
        for result in all_search_results:
            url = result.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(result)
        
        if self.verbose:
            print(f"[æœç´¢å¢å¼º] å…±è·å– {len(unique_results)} æ¡å”¯ä¸€ç»“æœ")
        
        # Step 3: ä½¿ç”¨ LLM ç”Ÿæˆå¢å¼ºä»»åŠ¡
        if self.verbose:
            print("[æœç´¢å¢å¼º] ç”Ÿæˆå¢å¼ºä»»åŠ¡...")
        enhanced_task, source_urls = await self._generate_enhanced_task(task, unique_results)
        
        return enhanced_task, source_urls
    
    async def enhance(self, task: str, force_refresh: bool = False) -> Dict[str, Any]:
        """
        å¢å¼ºä»»åŠ¡æè¿°çš„ä¸»å…¥å£
        
        æµç¨‹ï¼š
        1. æœç´¢ç¼“å­˜ï¼ˆPostgreSQL å‘é‡åº“ï¼‰
        2. ç¼“å­˜å‘½ä¸­ -> ç›´æ¥è¿”å›ç¼“å­˜çš„ç­”æ¡ˆ
        3. ç¼“å­˜æœªå‘½ä¸­ -> è”ç½‘æœç´¢ -> LLM ç”Ÿæˆ -> å­˜å…¥ç¼“å­˜ -> è¿”å›ç»“æœ
        
        Args:
            task: åŸå§‹ä»»åŠ¡æè¿°
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ï¼ˆè·³è¿‡ç¼“å­˜ï¼‰
            
        Returns:
            åŒ…å«å¢å¼ºç»“æœçš„å­—å…¸ï¼š
            - enhanced_task: å¢å¼ºåçš„ä»»åŠ¡æè¿°
            - cache_hit: æ˜¯å¦å‘½ä¸­ç¼“å­˜
            - search_performed: æ˜¯å¦æ‰§è¡Œäº†è”ç½‘æœç´¢
            - source: ç»“æœæ¥æº ("cache" / "search" / "original")
            - source_urls: å¼•ç”¨çš„URLåˆ—è¡¨
        """
        if self.verbose:
            print(f"\n{'='*60}")
            print(f"ğŸ“ ä»»åŠ¡å¢å¼ºå¼€å§‹: {task[:100]}...")
            print(f"{'='*60}")
        
        result = {
            "original_task": task,
            "enhanced_task": task,
            "cache_hit": False,
            "search_performed": False,
            "source": "original",
            "source_urls": []
        }
        
        # Step 1: æœç´¢ç¼“å­˜ï¼ˆé™¤éå¼ºåˆ¶åˆ·æ–°ï¼‰
        if not force_refresh and self.enable_cache:
            if self.verbose:
                print("\n[Step 1] æœç´¢å‘é‡ç¼“å­˜...")
            cache_result = await self.search_cache(task)
            
            if cache_result:
                if self.verbose:
                    print(f"âœ“ å‘½ä¸­ç¼“å­˜ï¼ˆç›¸ä¼¼åº¦: {cache_result.get('similarity', 0):.4f}ï¼‰")
                result["enhanced_task"] = cache_result.get("answer_text", task)
                result["cache_hit"] = True
                result["source"] = "cache"
                result["cache_similarity"] = cache_result.get("similarity", 0)
                return result
        else:
            if self.verbose:
                if force_refresh:
                    print("âš¡ å¼ºåˆ¶åˆ·æ–°æ¨¡å¼ï¼Œè·³è¿‡ç¼“å­˜")
                else:
                    print("âš ï¸ ç¼“å­˜å·²ç¦ç”¨")
        
        # Step 2: ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œè”ç½‘æœç´¢ + LLM ç”Ÿæˆ
        if self.enable_web_search:
            if self.verbose:
                print("\n[Step 2] å¯åŠ¨è”ç½‘æœç´¢...")
            
            enhanced_task, source_urls = await self._execute_search_and_enhance(task)
            
            result["enhanced_task"] = enhanced_task
            result["search_performed"] = True
            result["source"] = "search"
            result["source_urls"] = source_urls
        else:
            if self.verbose:
                print("\nâš ï¸ è”ç½‘æœç´¢å·²ç¦ç”¨ï¼Œä½¿ç”¨åŸå§‹ä»»åŠ¡")
        
        # Step 3: å°†ç»“æœå­˜å…¥ç¼“å­˜
        if result["enhanced_task"] != task and self.enable_cache:
            if self.verbose:
                print("\n[Step 3] ä¿å­˜åˆ°å‘é‡ç¼“å­˜...")
            await self.save_to_cache(task, result["enhanced_task"])
        
        if self.verbose:
            print(f"\nâœ“ ä»»åŠ¡å¢å¼ºå®Œæˆ")
            print(f"{'='*60}\n")
        
        return result


class FeishuEnhancerConfig:
    """é£ä¹¦å¢å¼ºå™¨é…ç½®ç±»"""
    
    def __init__(self):
        # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶åŠ è½½
        try:
            import config
            self.bocha_api_key = config.config_dict.get("BOCHA_API_KEY", os.getenv("BOCHA_API_KEY", ""))
            self.bocha_base_url = config.config_dict.get("BOCHA_BASE_URL", "https://api.bochaai.com/v1")
            self.similarity_threshold = float(config.config_dict.get("ENHANCE_SIMILARITY_THRESHOLD", "0.85"))
            self.enable_cache = str(config.config_dict.get("ENHANCE_ENABLE_CACHE", "true")).lower() == "true"
            self.enable_web_search = str(config.config_dict.get("ENHANCE_ENABLE_WEB_SEARCH", "true")).lower() == "true"
        except ImportError:
            self.bocha_api_key = os.getenv("BOCHA_API_KEY", "")
            self.bocha_base_url = os.getenv("BOCHA_BASE_URL", "https://api.bochaai.com/v1")
            self.similarity_threshold = float(os.getenv("ENHANCE_SIMILARITY_THRESHOLD", "0.85"))
            self.enable_cache = os.getenv("ENHANCE_ENABLE_CACHE", "true").lower() == "true"
            self.enable_web_search = os.getenv("ENHANCE_ENABLE_WEB_SEARCH", "true").lower() == "true"


# å…¨å±€å¢å¼ºå™¨å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
_enhancer_instance: Optional[FeishuTaskEnhancer] = None


def get_feishu_enhancer(llm: ChatOpenAI) -> FeishuTaskEnhancer:
    """
    è·å–é£ä¹¦å¢å¼ºå™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    
    Args:
        llm: LangChain ChatOpenAI å®ä¾‹
        
    Returns:
        FeishuTaskEnhancer å®ä¾‹
    """
    global _enhancer_instance
    if _enhancer_instance is None:
        config = FeishuEnhancerConfig()
        _enhancer_instance = FeishuTaskEnhancer(
            llm=llm,
            bocha_api_key=config.bocha_api_key,
            bocha_base_url=config.bocha_base_url,
            similarity_threshold=config.similarity_threshold,
            enable_cache=config.enable_cache,
            enable_web_search=config.enable_web_search
        )
    return _enhancer_instance